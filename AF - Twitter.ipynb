{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP - Avaliação Final - Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1: Produto e base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeira leitura dos dados\n",
    "\n",
    "df_training = pd.read_csv('twitter_sentiment/twitter_training.csv', names = ['id','theme','sentiment','text'])\n",
    "df_test = pd.read_csv('twitter_sentiment/twitter_validation.csv', names = ['id','theme','sentiment','text'])\n",
    "df_training.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames = [df_training, df_test]\n",
    "df_join = pd.concat(frames)\n",
    "df_cleaned = df_join.dropna()\n",
    "df_join.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2: Estratégias de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2A - Abordagem tradicional 'baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Classificador + Vetorizador\n",
    "\n",
    "n_components = 3\n",
    "X_train, X_test, y_train, y_test= train_test_split(df_join['text'].values.astype('U'), df_join['sentiment'].values.astype('U'), train_size=0.70)\n",
    "classificador = Pipeline([\n",
    "                        ('meu_vetorizador', CountVectorizer(stop_words='english')),\n",
    "                        ('meu_classificador', LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000))\n",
    "                        ])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "classificador.fit(X_train,y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Time taken by the pipeline: {:.2f} seconds\".format(elapsed_time))\n",
    "\n",
    "joblib.dump(classificador, 'NLP_AF_A.joblib')\n",
    "y_pred = classificador.predict(X_test)\n",
    "acc = accuracy_score(y_pred,y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"NLP_AF_A.joblib\")\n",
    "y_pred = list(model.predict_proba(['I hate Brazil'])[0])\n",
    "classification = y_pred.index(max(y_pred))\n",
    "return_class = model.classes_[classification]\n",
    "return_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "y_pred = classificador.predict_proba([\"I enjoyed my weekend\"])\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Time taken by the pipeline: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocabulario = classificador['meu_vetorizador'].vocabulary_\n",
    "pesos = classificador['meu_classificador'].coef_\n",
    "print(pesos.shape)\n",
    "\n",
    "classe_alvo = 1\n",
    "classe_alvo_str = classificador.classes_[classe_alvo]\n",
    "\n",
    "palavras_e_pesos = []\n",
    "for palavra in vocabulario.keys():\n",
    "    j = vocabulario[palavra]\n",
    "    coeficiente = pesos[classe_alvo,j]\n",
    "    palavras_e_pesos.append( (coeficiente, palavra) )\n",
    "\n",
    "tuplas_ordenadas = sorted(palavras_e_pesos, reverse=True) # reverse=True pede uma ordenação em ordem decrescente\n",
    "palavras = [ t[1] for t in tuplas_ordenadas ]\n",
    "contagens = [ t[0] for t in tuplas_ordenadas ]\n",
    "\n",
    "n_palavras = 10\n",
    "eixo_x = np.arange(n_palavras)\n",
    "plt.figure(figsize=(14,1))\n",
    "plt.title('Palavras que mais levam a {}'.format(classificador.classes_[classe_alvo]))\n",
    "plt.bar(eixo_x[0:n_palavras], contagens[0:n_palavras])\n",
    "plt.xticks(eixo_x[0:n_palavras], palavras[0:n_palavras], rotation=20, fontsize = 12)\n",
    "plt.ylabel(f'Pesos do regressor\\nlogístico')\n",
    "plt.show()\n",
    "\n",
    "eixo_x = np.arange(n_palavras)\n",
    "plt.figure(figsize=(14,1))\n",
    "plt.title('Palavras que mais afastam de {}'.format(classificador.classes_[classe_alvo]))\n",
    "plt.bar(eixo_x[-n_palavras:], contagens[-n_palavras:])\n",
    "plt.xticks(eixo_x[-n_palavras:], palavras[-n_palavras:], rotation=20, fontsize = 12)\n",
    "plt.ylabel(f'Pesos do regressor\\nlogístico')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2B - Abordagem com Deep Learning treinada in-house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas - rede neural\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, TimeDistributed, Lambda, Softmax, TextVectorization, Reshape, RepeatVector, GRU, Conv1D, Bidirectional, AveragePooling1D, UpSampling1D, Embedding, Concatenate, GlobalAveragePooling1D, LSTM, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão dos databases em minibatches\n",
    "df = df_join\n",
    "DATASET_DIR = './twitter_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Não rodar novamente\n",
    "# os.mkdir(DATASET_DIR)\n",
    "# os.mkdir(DATASET_DIR + \"/train\")\n",
    "# os.mkdir(DATASET_DIR + \"/train/Positive\")\n",
    "# os.mkdir(DATASET_DIR + \"/train/Negative\")\n",
    "# os.mkdir(DATASET_DIR + \"/train/Irrelevant\")\n",
    "# os.mkdir(DATASET_DIR + \"/train/Neutral\")\n",
    "# os.mkdir(DATASET_DIR + \"/test\")\n",
    "# os.mkdir(DATASET_DIR + \"/test/Positive\")\n",
    "# os.mkdir(DATASET_DIR + \"/test/Negative\")\n",
    "# os.mkdir(DATASET_DIR + \"/test/Irrelevant\")\n",
    "# os.mkdir(DATASET_DIR + \"/test/Neutral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = df['text']\n",
    "# labels = df['sentiment']\n",
    "# x_train, x_test, y_train, y_test = train_test_split(reviews, labels, train_size=0.8)\n",
    "\n",
    "# n_texto = 0\n",
    "# for i in tqdm(range(len(y_train))):\n",
    "#     texto = x_train.iloc[i]\n",
    "#     fname = 'review_' + str(n_texto) + '.txt'\n",
    "#     with open(DATASET_DIR + \"/train/\" + y_train.iloc[i] + \"/\" + fname, 'w', encoding = 'utf-8') as f:\n",
    "#         f.write(str(texto))\n",
    "#         n_texto += 1\n",
    "\n",
    "# for i in tqdm(range(len(y_test))):\n",
    "#     texto = x_test.iloc[i]\n",
    "#     fname = 'review_' + str(n_texto) + '.txt'\n",
    "#     with open(DATASET_DIR + \"/test/\" + y_test.iloc[i] + \"/\" + fname, 'w', encoding = 'utf-8') as f:\n",
    "#         f.write(str(texto))\n",
    "#         n_texto += 1\n",
    "\n",
    "# print(\"Criei textos:\", n_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler um dataset e fazer batches\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "\n",
    "dataset_train = text_dataset_from_directory(\n",
    "    DATASET_DIR + '/train',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=None,\n",
    "    batch_size=1024,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    follow_links=False\n",
    ")\n",
    "\n",
    "dataset_test = text_dataset_from_directory(\n",
    "    DATASET_DIR+ '/test',\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    class_names=None,\n",
    "    batch_size=1024,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    follow_links=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, TextVectorization\n",
    "from keras.models import Model\n",
    "\n",
    "def remover_label(x,label):\n",
    "    return x\n",
    "\n",
    "vocab_size = 10000\n",
    "vectorize_layer = TextVectorization(max_tokens=vocab_size, output_sequence_length=256)\n",
    "vectorize_layer.adapt(dataset_train.map(remover_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convolve_and_downsample(input_n_samples, input_embedding_size, n_filters, kernel_size=3, **kwargs):\n",
    "    input_layer = Input(shape=(input_n_samples,input_embedding_size))\n",
    "    x = input_layer\n",
    "    x = Conv1D( filters=n_filters,\n",
    "                kernel_size=kernel_size,\n",
    "                padding='same',\n",
    "                use_bias=False,\n",
    "                )(x)\n",
    "    x = AveragePooling1D(pool_size=2)(x)\n",
    "    x = Activation('elu')(x)\n",
    "    return Model(input_layer, x, **kwargs)\n",
    "\n",
    "def deep_cnn_embedding_softmax_model(vectorize_layer, vocab_size=vocab_size, number_of_ngrams=16, n_gram_size=3):\n",
    "    input_layer = Input(shape=(1,), dtype=tf.string)\n",
    "    x = input_layer\n",
    "    x = vectorize_layer(x)\n",
    "    x = Embedding(vocab_size, 2, name='projecao')(x)\n",
    "    x = convolve_and_downsample(256, 2, number_of_ngrams, n_gram_size, name='ngramas')(x)\n",
    "    x = convolve_and_downsample(128, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "    x = convolve_and_downsample(64, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "    x = convolve_and_downsample(32, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "    x = convolve_and_downsample(16, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "    x = convolve_and_downsample(8, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "    x = convolve_and_downsample(4, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "    x = convolve_and_downsample(2, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "    x = Reshape( (-1,))(x)\n",
    "    x = Dense(4, name='classificador')(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    return Model(input_layer, x)\n",
    "\n",
    "clf = deep_cnn_embedding_softmax_model(vectorize_layer)\n",
    "print(clf.summary())\n",
    "clf.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "history = clf.fit(dataset_train, epochs=30, verbose=1, validation_data=dataset_test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Time taken by the pipeline: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convolve_and_downsample(input_n_samples, input_embedding_size, n_filters, kernel_size=3, **kwargs):\n",
    "#     input_layer = Input(shape=(input_n_samples,input_embedding_size))\n",
    "#     x = input_layer\n",
    "#     x = Conv1D( filters=n_filters,\n",
    "#                 kernel_size=kernel_size,\n",
    "#                 padding='same',\n",
    "#                 use_bias=False,\n",
    "#                 )(x)\n",
    "#     x = AveragePooling1D(pool_size=2)(x)\n",
    "#     x = Activation('elu')(x)\n",
    "#     return Model(input_layer, x, **kwargs)\n",
    "\n",
    "# def deep_cnn_embedding_softmax_model(vectorize_layer, vocab_size=vocab_size, number_of_ngrams=16, n_gram_size=3):\n",
    "#     input_layer = Input(shape=(1,), dtype=tf.string)\n",
    "#     x = input_layer\n",
    "#     x = vectorize_layer(x)\n",
    "#     x = Embedding(vocab_size, 2, name='projecao')(x)\n",
    "#     x = convolve_and_downsample(256, 2, number_of_ngrams, n_gram_size, name='ngramas')(x)\n",
    "#     x = convolve_and_downsample(128, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "#     x = convolve_and_downsample(64, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "#     x = convolve_and_downsample(32, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "#     x = convolve_and_downsample(16, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "#     x = convolve_and_downsample(8, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "#     x = convolve_and_downsample(4, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "#     x = convolve_and_downsample(2, number_of_ngrams, number_of_ngrams, n_gram_size)(x)\n",
    "#     x = Reshape( (-1,))(x)\n",
    "#     x = Dense(4, name='classificador')(x)\n",
    "#     x = Activation('softmax')(x)\n",
    "#     return Model(input_layer, x)\n",
    "\n",
    "# for i in range(0,10):\n",
    "#     clf = deep_cnn_embedding_softmax_model(vectorize_layer)\n",
    "#     print(clf.summary())\n",
    "#     clf.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     start_time = time.time()\n",
    "\n",
    "#     history = clf.fit(dataset_train, epochs=30, verbose=1, validation_data=dataset_test)\n",
    "\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     elapsed_time = end_time - start_time\n",
    "\n",
    "#     print(\"Time taken by the pipeline: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.evaluate(dataset_test)\n",
    "# clf.save('NLP_AF_B')\n",
    "    \n",
    "from tensorflow import keras\n",
    "clf = keras.models.load_model('NLP_AF_B')\n",
    "# clf.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,1))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure(figsize=(14,1))\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2C - Abordagem com Deep Learning  utilizando rede pré-treinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Item 2C\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# max_len = 1000\n",
    "\n",
    "# def embed_text(text):\n",
    "#     response = tokenizer(text, truncation = True, padding = True, return_tensors='tf')\n",
    "#     return model(response)[0][:,0,:]\n",
    "\n",
    "# # Apply the embedding function to the 'text_column' in your DataFrame\n",
    "# list_embed = [0]*df.shape[0]\n",
    "# for index, row in tqdm(df[74682:].iterrows()):\n",
    "#     text = str(row['text'])\n",
    "#     embeded_text = embed_text(text).numpy()[0]\n",
    "#     list_embed[index] = embeded_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('bert_array_74682_end', list_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classificador = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
    "\n",
    "# classificador.fit(list_embed[0:int(max_len*0.6)],y_train[0:int(max_len*0.6)])\n",
    "# y_pred = classificador.predict(list_embed[int(max_len*0.6):max_len])\n",
    "# acc = accuracy_score(y_pred,y_train[int(max_len*0.6):max_len])\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bert_array_1 = np.load('bert_array_24003.npy', allow_pickle=True)[0:24004]\n",
    "bert_array_2 = np.load('bert_array_24004_74682.npy', allow_pickle=True)[24004:74682]\n",
    "bert_array_3 = np.load('bert_array_74682_end.npy', allow_pickle=True)[:1000]\n",
    "bert_array = list(np.concatenate((bert_array_1, bert_array_2,bert_array_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['sentiment'])\n",
    "\n",
    "classificador = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(bert_array, y, train_size=0.70)\n",
    "\n",
    "#lower_bound = int(0.6*len(bert_array))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "classificador.fit(X_train,y_train)\n",
    "y_pred = classificador.predict(X_test)\n",
    "acc = accuracy_score(y_pred,y_test)\n",
    "print(acc)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Time taken by the pipeline: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "y_pred = classificador.predict([X_test[0]])\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"Time taken by the pipeline: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D - Abordagem com rede pré treinada com mínimo de pós processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2D - Rede pré treinada com mínimo de pós processamento\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# TF\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def roberta_twitter_sentiment(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='tf')\n",
    "    output = model(encoded_input)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    \n",
    "#     for i in range(scores.shape[0]):\n",
    "#         l = labels[ranking[i]]\n",
    "#         s = scores[ranking[i]]\n",
    "#         print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    \n",
    "    scores = scores.tolist()\n",
    "    max_index = scores.index(max(scores))\n",
    "    return \"{0}\".format(labels[max_index])\n",
    "\n",
    "roberta_twitter_sentiment('Enjoy!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_list = []\n",
    "max_len = 500\n",
    "for index,row in tqdm(df.iterrows()):\n",
    "    roberta_list.append(roberta_twitter_sentiment(str(row['text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('roberta_list', roberta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_values(array):\n",
    "    for i in range(len(array)):\n",
    "        if array[i] == \"Irrelevant\":\n",
    "            array[i] = \"Neutral\"\n",
    "        if array[i] == \"positive\":\n",
    "            array[i] = \"Positive\"\n",
    "        if array[i] == \"negative\":\n",
    "            array[i] = \"Negative\"\n",
    "        if array[i] == \"neutral\":\n",
    "            array[i] = \"Neutral\"\n",
    "    return array\n",
    "\n",
    "y = np.array(df['sentiment'])\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(roberta_list, y, train_size=0.70)\n",
    "\n",
    "y_test_roberta = change_values(y_test)\n",
    "roberta_list = change_values(roberta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(roberta_list, y)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
